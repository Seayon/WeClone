# WeClone 批量并行处理功能使用说明

## 功能概述

WeClone 现在支持通过命令行参数指定输入和输出路径，允许您通过多开命令行窗口来实现并行处理不同数据集。

## 新增命令行参数

`make-dataset` 命令现在支持以下参数：

- `--input-dir` / `-i`: 指定输入CSV文件目录（默认: `./dataset/csv`）
- `--output-dir` / `-o`: 指定输出目录（默认: `./dataset/res_csv/sft`）
- `--output-file` / `-f`: 指定输出文件名（默认: `sft-my.json`）

## 使用方法

### 1. 默认方式运行
```bash
python -m weclone make-dataset
```
这将使用默认的输入输出路径。

### 2. 指定输入和输出目录
```bash
python -m weclone make-dataset --input-dir ./dataset/csv/user1 --output-dir ./dataset/res_csv/user1
```

### 3. 使用简写参数
```bash
python -m weclone make-dataset -i ./dataset/csv/user2 -o ./dataset/res_csv/user2 -f user2.json
```

### 4. 并行处理多个数据集

您可以同时打开多个命令行窗口，每个窗口处理不同的数据集：

**窗口1:**
```bash
python -m weclone make-dataset -i ./dataset/csv/dataset1 -o ./output/dataset1 -f dataset1.json
```

**窗口2:**
```bash
python -m weclone make-dataset -i ./dataset/csv/dataset2 -o ./output/dataset2 -f dataset2.json
```

**窗口3:**
```bash
python -m weclone make-dataset -i ./dataset/csv/dataset3 -o ./output/dataset3 -f dataset3.json
```

## 目录结构建议

为了更好地组织您的数据，建议使用以下目录结构：

```
WeClone/
├── dataset/
│   ├── csv/
│   │   ├── user1/          # 用户1的聊天记录CSV文件
│   │   ├── user2/          # 用户2的聊天记录CSV文件
│   │   └── user3/          # 用户3的聊天记录CSV文件
│   └── output/
│       ├── user1/          # 用户1的处理结果
│       ├── user2/          # 用户2的处理结果
│       └── user3/          # 用户3的处理结果
```

## 注意事项

1. **输入目录要求**: 输入目录必须存在且包含CSV文件，否则程序会报错退出。

2. **输出目录自动创建**: 如果指定的输出目录不存在，程序会自动创建。

3. **文件覆盖**: 如果输出文件已存在，程序会覆盖原文件。

4. **配置文件**: 程序仍然使用 `settings.jsonc` 中的配置，只是输入输出路径可以通过命令行参数覆盖。

5. **并行处理**: 每个命令行窗口是独立的进程，可以同时运行多个实例而不会相互干扰。

## 实际使用示例

假设您有3个用户的聊天记录需要处理：

```bash
# 在第一个命令行窗口
python -m weclone make-dataset -i ./data/user_alice -o ./results/alice -f alice_qa.json

# 在第二个命令行窗口  
python -m weclone make-dataset -i ./data/user_bob -o ./results/bob -f bob_qa.json

# 在第三个命令行窗口
python -m weclone make-dataset -i ./data/user_charlie -o ./results/charlie -f charlie_qa.json
```

这样就可以同时处理三个用户的数据，大大提高处理效率。

## 查看帮助信息

您可以使用以下命令查看完整的参数说明：

```bash
python -m weclone make-dataset --help
```

## 问题解决

### dataset_info.json 文件缺失问题

当使用自定义输出目录时，程序会自动创建所需的 `dataset_info.json` 配置文件，确保后续的 length_cdf 计算能够正常运行。

### 进程被 killed 的问题

如果在并行处理时遇到进程被 killed 的情况，可能的原因包括：

1. **内存不足**: 多个进程同时处理大量数据可能消耗过多内存
2. **文件冲突**: 多个进程同时访问相同的配置文件或模型文件
3. **系统资源限制**: CPU 负载过高或文件句柄数量超限
4. **length_cdf 脚本错误**: 当使用自定义输出目录时，可能因为缺少 dataset_info.json 文件而失败

**建议解决方案**:
- 减少同时运行的进程数量
- 监控系统资源使用情况 (`htop`, `nvidia-smi`)
- 确保每个输出目录都有足够的磁盘空间
- 如果问题持续，考虑顺序执行而非并行执行